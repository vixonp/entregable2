import os
import pandas as pd
import subprocess
import time # Añadido para esperas

# ==============================================================================
# Configuración
# ==============================================================================
PIG_CONTAINER_NAME = "pig_container"
PIG_WORKING_DIR_CONTAINER = "/data" # Donde se monta ./data del host
HDFS_INPUT_DIR = "/user/hadoop/input" # Directorio de HDFS para entrada
HDFS_OUTPUT_ROOT_DIR = "/user/hadoop" # Directorio de HDFS para salida
JAVA_HOME = "/usr/lib/jvm/java-8-openjdk-amd64" # JAVA_HOME dentro del contenedor

# ==============================================================================
# Funciones auxiliares
# ==============================================================================

def ejecutar_comando_docker(comando):
    """Ejecuta un comando de Docker y retorna la salida o el error."""
    try:
        resultado = subprocess.run(comando, capture_output=True, text=True, check=True)
        return True, resultado.stdout.strip()
    except subprocess.CalledProcessError as e:
        return False, e.stderr.strip()
    except FileNotFoundError:
        return False, "Error: El comando 'docker' no se encontró. Asegúrate de tener Docker instalado y en el PATH."
    except Exception as e:
        return False, f"Error inesperado al ejecutar comando Docker: {e}"

def convertir_excel_a_csv(ruta_excel="data/eventos.xlsx", ruta_csv="data/eventos.csv"):
    """Convierte un archivo Excel a CSV."""
    print(f" Convirtiendo {ruta_excel} a {ruta_csv}...")
    if not os.path.exists(ruta_excel):
        print(f" Error: Archivo Excel de entrada no encontrado: {ruta_excel}")
        return False
    try:
        df = pd.read_excel(ruta_excel)
        df.to_csv(ruta_csv, index=False)
        print(f" Conversión exitosa. CSV guardado en {ruta_csv}")
        return True
    except Exception as e:
        print(f" Error al convertir Excel a CSV: {e}")
        return False

def check_container_running(nombre_contenedor):
    success, output = ejecutar_comando_docker(["docker", "ps", "--filter", f"name={nombre_contenedor}", "--format", "{{.Names}}"])
    return success and nombre_contenedor in output

def ejecutar_hdfs_comando(comando_hdfs):
    """Ejecuta un comando HDFS dentro del contenedor Pig."""
    comando = ["docker", "exec", PIG_CONTAINER_NAME, "hdfs", "dfs"] + comando_hdfs
    print(f"Executing HDFS command: {' '.join(comando_hdfs)}")
    return ejecutar_comando_docker(comando)

def cargar_csv_a_hdfs(local_csv_path="data/eventos.csv", hdfs_path=f"{HDFS_INPUT_DIR}/eventos.csv"):
    """Copia el archivo CSV del host a HDFS."""
    print(f" Cargando {local_csv_path} a HDFS ({hdfs_path})...")
    if not os.path.exists(local_csv_path):
        print(f" Error: CSV de entrada no encontrado en el host: {local_csv_path}")
        return False

    # Crear el directorio de entrada en HDFS si no existe
    ejecutar_hdfs_comando(["-mkdir", "-p", HDFS_INPUT_DIR])

    # Copiar el archivo del sistema de archivos local del contenedor a HDFS
    # Primero copiamos el archivo del host al /data del contenedor (gracias al volumen)
    # Luego, desde /data en el contenedor, lo copiamos a HDFS
    container_local_path = f"{PIG_WORKING_DIR_CONTAINER}/eventos.csv"
    
    # Asegúrate de que el archivo existe en la ruta local del contenedor
    # (Ya se verificó al inicio de la función ejecutar_pig_paralelo con verificar_csv_en_contenedor,
    # pero aquí nos aseguramos antes de mover a HDFS)
    
    # Eliminar archivo previo en HDFS para evitar duplicados si existe
    ejecutar_hdfs_comando(["-rm", "-f", hdfs_path])
    
    success, output = ejecutar_hdfs_comando(["-put", container_local_path, hdfs_path])
    if not success:
        print(f" Error al cargar CSV a HDFS: {output}")
        return False
    print(f" CSV cargado a HDFS: {hdfs_path}")
    return True

def eliminar_hdfs_output(hdfs_path):
    """Elimina un directorio de salida en HDFS."""
    print(f" Eliminando output previo en HDFS: {hdfs_path}...")
    success, output = ejecutar_hdfs_comando(["-rm", "-r", "-skipTrash", hdfs_path])
    if not success and "No such file or directory" not in output: # Ignorar si no existe
        print(f" Advertencia: No se pudo eliminar el directorio de salida HDFS '{hdfs_path}'. Error: {output}")
    else:
        print(f" Output HDFS previo eliminado (o no existía).")

def copiar_resultado_de_hdfs(hdfs_source_path, local_destination_path):
    """Copia el resultado de Pig desde HDFS al host."""
    print(f" Copiando resultado de HDFS ({hdfs_source_path}) a {local_destination_path}...")
    
    if not os.path.exists(os.path.dirname(local_destination_path)):
        os.makedirs(os.path.dirname(local_destination_path))

    # Eliminar cualquier archivo o carpeta preexistente en la ruta de destino local
    if os.path.exists(local_destination_path):
        if os.path.isdir(local_destination_path):
            try:
                import shutil
                shutil.rmtree(local_destination_path)
            except Exception as e:
                print(f" Error al eliminar directorio local {local_destination_path}: {e}")
                return False
        else:
            try:
                os.remove(local_destination_path)
            except Exception as e:
                print(f" Error al eliminar archivo local {local_destination_path}: {e}")
                return False
    
    # Copiar del HDFS al /data del contenedor, luego del /data al host
    # Hadoop tiene un comando get que ya hace esto para directorios.
    # El comando -get copiará el directorio entero con sus partes.
    success, output = ejecutar_hdfs_comando(["-get", hdfs_source_path, f"{PIG_WORKING_DIR_CONTAINER}/"])
    
    if not success:
        print(f" Error al copiar resultado de HDFS a contenedor local: {output}")
        return False
    
    # Mover el archivo part-r-00000 de la carpeta copiada a la ruta final en el host
    # El -get comando crea una carpeta con el nombre de hdfs_source_path (ej. output_tipo_calle.csv)
    # dentro de /data. Necesitamos acceder a part-r-00000 dentro de esa carpeta.
    copied_dir_in_container_name = os.path.basename(hdfs_source_path) # ej. output_tipo_calle.csv
    container_part_file_path = f"{PIG_WORKING_DIR_CONTAINER}/{copied_dir_in_container_name}/part-r-00000"
    
    # Ahora, copiamos el part-r-00000 de vuelta al destino local del host
    comando_cp_to_host = ["docker", "cp", f"{PIG_CONTAINER_NAME}:{container_part_file_path}", local_destination_path]
    success_cp, output_cp = ejecutar_comando_docker(comando_cp_to_host)

    if not success_cp:
        print(f" Error al copiar part-r-00000 de contenedor al host: {output_cp}")
        return False

    print(f" Resultado copiado a {local_destination_path}")
    return True


def ejecutar_pig_paralelo():
    print("\n Iniciando procesamiento distribuido con Pig sobre Hadoop...")

    if not check_container_running(PIG_CONTAINER_NAME):
        print(f" Error: El contenedor '{PIG_CONTAINER_NAME}' no está corriendo. Inícialo primero.")
        print("Asegúrate de ejecutar 'docker-compose up -d' y esperar a que Hadoop inicie.")
        return

    # Dar un tiempo para que Hadoop inicie dentro del contenedor (puede tardar unos segundos)
    print(" Esperando que Hadoop inicie completamente en el contenedor...")
    time.sleep(15) # Ajusta este tiempo si es necesario, 15-30 segundos suele ser suficiente

    # 1. Convertir Excel a CSV (en el host)
    local_input_csv = "data/eventos.csv"
    if not convertir_excel_a_csv(ruta_csv=local_input_csv): 
        print("Proceso detenido debido a error en conversión de Excel a CSV.")
        return

    # 2. Copiar CSV al HDFS (Desde el volumen compartido, a HDFS)
    #    Pig leerá desde HDFS, no desde el sistema de archivos local del contenedor.
    hdfs_input_file = f"{HDFS_INPUT_DIR}/eventos.csv"
    if not cargar_csv_a_hdfs(local_csv_path=local_input_csv, hdfs_path=hdfs_input_file):
        print("Proceso detenido porque falló la carga del CSV a HDFS.")
        return

    scripts = [
        {
            "script": "test.pig",
            "hdfs_output_dir": "output_tipo_calle.csv", # Será /user/hadoop/output_tipo_calle.csv en HDFS
            "result_file": "data/output_tipo_calle.csv", # Ruta final en el host
            "columnas": ["tipo", "calle", "cantidad"]
        },
        {
            "script": "test_hora.pig",
            "hdfs_output_dir": "output_por_hora.csv", # Será /user/hadoop/output_por_hora.csv en HDFS
            "result_file": "data/output_por_hora.csv", # Ruta final en el host
            "columnas": ["hora", "cantidad"]
        }
    ]

    procesos = []

    for cfg in scripts:
        script_in_container_path = f"{PIG_WORKING_DIR_CONTAINER}/{cfg['script']}" # Ruta del script en el volumen del contenedor
        local_script_path = f"data/{cfg['script']}" # Ruta del script en el host
        hdfs_final_output_path = f"{HDFS_OUTPUT_ROOT_DIR}/{cfg['hdfs_output_dir']}" # Ruta de salida en HDFS
        log_path = f"data/{cfg['script']}.log" # Ruta del log en el host
        
        # Copiar script Pig al contenedor (a /data), si no confías solo en el volumen
        # Con el volumen ./data:/data, el script ya está en /data/test.pig
        # Si prefieres ser explícito:
        # success_cp_script, _ = ejecutar_comando_docker(["docker", "cp", local_script_path, f"{PIG_CONTAINER_NAME}:{script_in_container_path}"])
        # if not success_cp_script:
        #     print(f" Falló la copia del script: {cfg['script']}. Proceso para este script detenido.")
        #     continue

        # Verificar que el script existe en el contenedor (accesible via volumen)
        success_check_script, output_check_script = ejecutar_comando_docker(["docker", "exec", PIG_CONTAINER_NAME, "test", "-f", script_in_container_path])
        if not success_check_script:
            print(f" Error: Script '{script_in_container_path}' no encontrado en el contenedor '{PIG_CONTAINER_NAME}'.")
            print(f"Asegúrate de que '{local_script_path}' existe en tu host y el volumen está bien mapeado.")
            continue


        # Eliminar output previo en HDFS
        eliminar_hdfs_output(hdfs_final_output_path)

        # Ejecutar Pig sobre Hadoop (sin -x local)
        comando_pig_exec = [
            "docker", "exec", "-i", PIG_CONTAINER_NAME,
            "bash", "-c",
            f"export JAVA_HOME={JAVA_HOME} && pig {script_in_container_path}"
        ]
        
        print(f" Ejecutando Pig para {cfg['script']} sobre Hadoop...")
        log_file = open(log_path, "w", encoding="utf-8")
        proc = subprocess.Popen(comando_pig_exec, stdout=log_file, stderr=subprocess.STDOUT, text=True)
        procesos.append((proc, log_file, cfg))

    # Esperar y procesar resultados
    for proc, log_file, cfg in procesos:
        proc.wait() # Espera a que el proceso Pig termine
        log_file.close()
        print(f" {cfg['script']} completado. Log guardado en: {log_file.name}")

        # Después de que Pig termina, la salida está en HDFS
        hdfs_source_path = f"{HDFS_OUTPUT_ROOT_DIR}/{cfg['hdfs_output_dir']}"
        final_csv_on_host = cfg["result_file"] # Ruta del CSV final en el host
        final_xlsx_on_host = final_csv_on_host.replace(".csv", ".xlsx") # Ruta del XLSX final en el host

        # Copiar el resultado de Pig desde HDFS al host
        if not copiar_resultado_de_hdfs(hdfs_source_path, final_csv_on_host):
            print(f" No se pudo copiar el resultado de HDFS para {cfg['script']}. Revisa los logs.")
            continue

        # Convertir el CSV de resultado a Excel
        if os.path.exists(final_csv_on_host):
            try:
                # Pig por defecto usa tabulador para la salida. Asegúrate de que tus scripts Pig
                # están usando PigStorage(',') o cambia sep="\t" aquí.
                df = pd.read_csv(final_csv_on_host, sep="\t", names=cfg["columnas"]) 
                df.to_excel(final_xlsx_on_host, index=False)
                print(f" Exportado a Excel: {final_xlsx_on_host}")
            except Exception as e:
                print(f" Error exportando {cfg['script']} a Excel: {e}. Revisa el formato del CSV de salida de Pig o el separador en read_csv.")
        else:
            print(f" No se encontró el archivo CSV de salida en el host para {cfg['script']}: {final_csv_on_host}")

# Las funciones mostrar_resultado_hora y mostrar_resultado_pig no cambian,
# ya que leen los XLSX generados por ejecutar_pig_paralelo().
def mostrar_resultado_hora():
    archivo_xlsx = "data/output_por_hora.xlsx" 
    if not os.path.exists(archivo_xlsx):
        print(" El archivo de evolución por hora aún no ha sido generado (o convertido a XLSX). Ejecuta Pig primero.")
        return

    try:
        df = pd.read_excel(archivo_xlsx)
        print(f" Resultado leído de Excel: {archivo_xlsx}\n")
        print(" Evolución temporal de eventos:")
        print(df)
    except Exception as e:
        print(f" Error al procesar los resultados por hora desde Excel: {e}")

def mostrar_resultado_pig():
    archivo_xlsx = "data/output_tipo_calle.xlsx" 
    if not os.path.exists(archivo_xlsx):
        print(" El archivo de resultados tipo-calle aún no ha sido generado (o convertido a XLSX). Ejecuta Pig primero.")
        return

    try:
        df = pd.read_excel(archivo_xlsx)
        print(f" Resultado leído de Excel: {archivo_xlsx}\n")
        print(" Resultados tipo-calle:")
        print(df.head())
    except Exception as e:
        print(f" Error al procesar los resultados tipo-calle desde Excel: {e}")